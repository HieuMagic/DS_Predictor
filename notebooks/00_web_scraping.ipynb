{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a7a6b6",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8698bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from queue import Queue\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9721c2",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP Headers\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Car brands list (113 brands)\n",
    "CAR_BRANDS = [\n",
    "    \"toyota\", \"hyundai\", \"kia\", \"ford\", \"honda\", \n",
    "    \"mazda\", \"mitsubishi\", \"vinfast\", \"chevrolet\", \n",
    "    \"nissan\", \"suzuki\",\n",
    "    \n",
    "    \"mercedes_benz\", \"bmw\", \"audi\", \"lexus\", \n",
    "    \"landrover\", \"porsche\", \"volvo\", \"peugeot\", \n",
    "    \"mini\", \"subaru\", \"volkswagen\",\n",
    "    \n",
    "    \"daewoo\", \n",
    "\n",
    "    \"isuzu\", \"jeep\",\n",
    "\n",
    "    \"mg\", \"baic\", \"wuling\", \"byd\" \n",
    "]\n",
    "\n",
    "# Brand mapping (URL code -> Display name)\n",
    "BRAND_MAPPING = {\n",
    "    \"toyota\": \"Toyota\",\n",
    "    \"honda\": \"Honda\",\n",
    "    \"mazda\": \"Mazda\",\n",
    "    \"mitsubishi\": \"Mitsubishi\",\n",
    "    \"nissan\": \"Nissan\",\n",
    "    \"suzuki\": \"Suzuki\",\n",
    "    \"subaru\": \"Subaru\",\n",
    "    \"isuzu\": \"Isuzu\",\n",
    "    \"lexus\": \"Lexus\",\n",
    "    \n",
    "    \"hyundai\": \"Hyundai\",\n",
    "    \"kia\": \"Kia\",\n",
    "    \"daewoo\": \"Daewoo\",\n",
    "    \"ssangyong\": \"Ssangyong\",\n",
    "    \n",
    "    \"ford\": \"Ford\",\n",
    "    \"chevrolet\": \"Chevrolet\",\n",
    "    \"jeep\": \"Jeep\",\n",
    "    \n",
    "    \"mercedes_benz\": \"Mercedes-Benz\",\n",
    "    \"bmw\": \"BMW\",\n",
    "    \"audi\": \"Audi\",\n",
    "    \"volkswagen\": \"Volkswagen\",\n",
    "    \"porsche\": \"Porsche\",\n",
    "    \"peugeot\": \"Peugeot\",\n",
    "    \"landrover\": \"Land Rover\",\n",
    "    \"volvo\": \"Volvo\",\n",
    "    \"mini\": \"MINI\",\n",
    "    \n",
    "    \"vinfast\": \"VinFast\",\n",
    "    \n",
    "    \"mg\": \"MG\",\n",
    "    \"baic\": \"BAIC\",\n",
    "    \"byd\": \"BYD\",\n",
    "    \"wuling\": \"Wuling\"\n",
    "}\n",
    "\n",
    "print(f\"âœ“ Total brands available: {len(CAR_BRANDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ad7f8",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_limit(delay: float = 0.0):\n",
    "    \"\"\"Add delay between requests.\"\"\"\n",
    "    time.sleep(delay)\n",
    "\n",
    "def get_headers() -> dict:\n",
    "    \"\"\"Get HTTP headers for requests.\"\"\"\n",
    "    return HEADERS.copy()\n",
    "\n",
    "def split_price(title: str) -> int:\n",
    "    \"\"\"\n",
    "    Extract price from Vietnamese title with Tá»· and Triá»‡u.\n",
    "    Example: '2 Tá»· 500 Triá»‡u' -> 2500000000 VND\n",
    "    \"\"\"\n",
    "    match_ty = re.search(r'(\\d+)\\s*Tá»·', title, re.IGNORECASE)\n",
    "    ty = int(match_ty.group(1)) if match_ty else 0\n",
    "    \n",
    "    match_trieu = re.search(r'(\\d+)\\s*Triá»‡u', title, re.IGNORECASE)\n",
    "    trieu = int(match_trieu.group(1)) if match_trieu else 0\n",
    "    \n",
    "    price = (ty * 1_000_000_000) + (trieu * 1_000_000)\n",
    "    return price\n",
    "\n",
    "def extract_model_from_title(title: str, brand: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract car model and year from title.\n",
    "    Returns: (model, year)\n",
    "    \"\"\"\n",
    "    # Remove price information\n",
    "    price_pattern = r'\\d+\\s*Tá»·|\\d+\\s*Triá»‡u'\n",
    "    price_match = re.search(price_pattern, title)\n",
    "    title_without_price = title[:price_match.start()].strip() if price_match else title\n",
    "    \n",
    "    # Remove brand name\n",
    "    brand_display = BRAND_MAPPING.get(brand, brand).replace('_', ' ')\n",
    "    title_lower = title_without_price.lower()\n",
    "    brand_lower = brand_display.lower()\n",
    "    \n",
    "    if title_lower.startswith(brand_lower):\n",
    "        remaining = title_without_price[len(brand_display):].strip()\n",
    "    else:\n",
    "        remaining = title_without_price\n",
    "    \n",
    "    # Extract year\n",
    "    year_pattern = r'\\b(19|20)\\d{2}\\b'\n",
    "    year_match = re.search(year_pattern, remaining)\n",
    "    \n",
    "    if year_match:\n",
    "        model = remaining[:year_match.start()].strip()\n",
    "        year = year_match.group()\n",
    "    else:\n",
    "        words = remaining.split()\n",
    "        model = ' '.join(words[:3]) if len(words) >= 3 else remaining\n",
    "        year = None\n",
    "    \n",
    "    return model, year\n",
    "\n",
    "def extract_engine_info(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract fuel type and engine capacity.\n",
    "    Example: 'XÄƒng 2.0 L' -> ('XÄƒng', '2.0')\n",
    "    \"\"\"\n",
    "    if \":\" in text:\n",
    "        value_part = text.split(\":\", 1)[1].strip()\n",
    "    else:\n",
    "        value_part = text.strip()\n",
    "    \n",
    "    match = re.search(r'^(.*?)\\s+(\\d+(?:\\.\\d+)?)\\s*L$', value_part, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip(), match.group(2).strip()\n",
    "    else:\n",
    "        return value_part, None\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5abe42",
   "metadata": {},
   "source": [
    "## 4. Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bonbanh_car_details(soup: BeautifulSoup, brand: str = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse car details from Bonbanh.com page.\n",
    "    Returns structured dictionary with 15 fields.\n",
    "    \"\"\"\n",
    "    car_data = {\n",
    "        'price': -1, 'brand': -1, 'model': -1, 'year': -1, 'odometer': -1,\n",
    "        'transmission': -1, 'fuel_type': -1, 'engine_capacity': -1,\n",
    "        'body_style': -1, 'origin': -1, 'seats': -1, 'condition': -1,\n",
    "        'num_owners': -1, 'inspection_status': -1, 'warranty_status': -1,\n",
    "        'source': 'bonbanh'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Extract title and price\n",
    "        title_div = soup.find('div', class_='title')\n",
    "        if title_div and title_div.find('h1'):\n",
    "            title = title_div.find('h1').text\n",
    "            title = re.sub(r'\\s+', ' ', title).strip()\n",
    "            \n",
    "            price = split_price(title)\n",
    "            car_data['price'] = price if price else -1\n",
    "            \n",
    "            if brand:\n",
    "                car_data['brand'] = BRAND_MAPPING.get(brand, brand)\n",
    "                model, year = extract_model_from_title(title, brand)\n",
    "                car_data['model'] = model if model else -1\n",
    "                car_data['year'] = year if year else -1\n",
    "        \n",
    "        # Extract details\n",
    "        detail_div = soup.find('div', class_='box_car_detail')\n",
    "        if detail_div:\n",
    "            mail_parents = detail_div.find_all('div', id='mail_parent')\n",
    "            \n",
    "            for mail_parent in mail_parents:\n",
    "                label_elem = mail_parent.find('label')\n",
    "                span_elem = mail_parent.find('span')\n",
    "                \n",
    "                if not label_elem or not span_elem:\n",
    "                    continue\n",
    "                \n",
    "                label = re.sub(r'\\s+', ' ', label_elem.text).strip().rstrip(':')\n",
    "                value = re.sub(r'\\s+', ' ', span_elem.text).strip()\n",
    "                \n",
    "                # Map labels to fields\n",
    "                if label == 'Sá»‘ Km Ä‘Ã£ Ä‘i':\n",
    "                    car_data['odometer'] = re.sub(r'\\D', '', value)\n",
    "                elif label == 'Há»™p sá»‘':\n",
    "                    car_data['transmission'] = value\n",
    "                elif label == 'Äá»™ng cÆ¡':\n",
    "                    fuel_type, engine_capacity = extract_engine_info(value)\n",
    "                    car_data['fuel_type'] = fuel_type if fuel_type else -1\n",
    "                    car_data['engine_capacity'] = engine_capacity if engine_capacity else -1\n",
    "                elif label == 'Kiá»ƒu dÃ¡ng':\n",
    "                    car_data['body_style'] = value\n",
    "                elif label == 'Xuáº¥t xá»©':\n",
    "                    car_data['origin'] = value\n",
    "                elif label == 'Sá»‘ chá»— ngá»“i':\n",
    "                    car_data['seats'] = value\n",
    "                elif label == 'TÃ¬nh tráº¡ng':\n",
    "                    car_data['condition'] = value\n",
    "                elif 'chá»§' in label.lower():\n",
    "                    car_data['num_owners'] = value\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {e}\")\n",
    "    \n",
    "    return car_data\n",
    "\n",
    "def parse_bonbanh_listing(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"Extract car detail URLs from Bonbanh listing page.\"\"\"\n",
    "    car_links = []\n",
    "    try:\n",
    "        car_lis = soup.find_all('li', class_='car-item')\n",
    "        for car_li in car_lis:\n",
    "            a_link = car_li.find('a')\n",
    "            if a_link and 'href' in a_link.attrs:\n",
    "                link = 'https://bonbanh.com/' + a_link.attrs['href']\n",
    "                car_links.append(link)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing listing: {e}\")\n",
    "    return car_links\n",
    "\n",
    "def parse_chotot_car_details(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"Parse car details from Chotot.com page.\"\"\"\n",
    "    car_data = {\n",
    "        'price': -1, 'brand': -1, 'model': -1, 'year': -1, 'odometer': -1,\n",
    "        'transmission': -1, 'fuel_type': -1, 'engine_capacity': -1,\n",
    "        'body_style': -1, 'origin': -1, 'seats': -1, 'condition': -1,\n",
    "        'num_owners': -1, 'inspection_status': -1, 'warranty_status': -1,\n",
    "        'source': 'chotot'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Extract price\n",
    "        div_title = soup.find('div', class_='cpmughi')\n",
    "        if div_title:\n",
    "            price_elem = div_title.find('b', class_='p26z2wb')\n",
    "            if price_elem:\n",
    "                price = re.sub(r'\\D', '', price_elem.text.strip())\n",
    "                car_data['price'] = price if price else -1\n",
    "        \n",
    "        # Extract other details\n",
    "        div_list = soup.find_all('div', class_='p1ja3eq0')\n",
    "        for div in div_list:\n",
    "            label_elem = div.find('span', class_='bwq0cbs')\n",
    "            if label_elem:\n",
    "                value_elem = label_elem.find_next_sibling('span')\n",
    "                if value_elem:\n",
    "                    label = label_elem.text.strip()\n",
    "                    value = value_elem.text.strip()\n",
    "                    \n",
    "                    # Map labels\n",
    "                    if label == 'Sá»‘ Km Ä‘Ã£ Ä‘i': car_data['odometer'] = value\n",
    "                    elif label == 'Há»™p sá»‘': car_data['transmission'] = value\n",
    "                    elif label == 'NhiÃªn liá»‡u': car_data['fuel_type'] = value\n",
    "                    elif label == 'Kiá»ƒu dÃ¡ng': car_data['body_style'] = value\n",
    "                    elif label == 'Xuáº¥t xá»©': car_data['origin'] = value\n",
    "                    elif label == 'Sá»‘ chá»—': car_data['seats'] = value\n",
    "                    elif label == 'TÃ¬nh tráº¡ng': car_data['condition'] = value\n",
    "                    elif label == 'HÃ£ng': car_data['brand'] = value\n",
    "                    elif label == 'NÄƒm sáº£n xuáº¥t': car_data['year'] = value\n",
    "                    elif label == 'DÃ²ng xe': car_data['model'] = value\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {e}\")\n",
    "    \n",
    "    return car_data\n",
    "\n",
    "def parse_chotot_listing(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"Extract car detail URLs from Chotot listing page.\"\"\"\n",
    "    car_links = []\n",
    "    base_url = 'https://xe.chotot.com'\n",
    "    try:\n",
    "        a_list = soup.find_all('a', class_='c15fd2pn')\n",
    "        for a in a_list:\n",
    "            if 'href' in a.attrs:\n",
    "                link = a.attrs['href']\n",
    "                if not link.startswith('http'):\n",
    "                    link = base_url + link\n",
    "                car_links.append(link)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing listing: {e}\")\n",
    "    return car_links\n",
    "\n",
    "print(\"âœ“ Parsing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066974a",
   "metadata": {},
   "source": [
    "## 5. Progress Tracker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressTracker:\n",
    "    \"\"\"Track crawling progress to enable resume functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, progress_file: str = \"../data/crawl_progress.json\"):\n",
    "        self.progress_file = Path(progress_file)\n",
    "        self.progress_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.lock = threading.Lock()\n",
    "        self.progress_data = self._load_progress()\n",
    "    \n",
    "    def _load_progress(self) -> Dict:\n",
    "        if self.progress_file.exists():\n",
    "            try:\n",
    "                with open(self.progress_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def _save_progress(self):\n",
    "        try:\n",
    "            with open(self.progress_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.progress_data, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save progress: {e}\")\n",
    "    \n",
    "    def get_last_page(self, source: str, brand: str = None) -> int:\n",
    "        key = f\"{source}_{brand}\" if brand else source\n",
    "        return self.progress_data.get(key, {}).get('last_page', 0)\n",
    "    \n",
    "    def update_page(self, source: str, page: int, brand: str = None):\n",
    "        with self.lock:\n",
    "            key = f\"{source}_{brand}\" if brand else source\n",
    "            if key not in self.progress_data:\n",
    "                self.progress_data[key] = {}\n",
    "            self.progress_data[key]['last_page'] = page\n",
    "            self._save_progress()\n",
    "    \n",
    "    def mark_completed(self, source: str, brand: str = None):\n",
    "        with self.lock:\n",
    "            key = f\"{source}_{brand}\" if brand else source\n",
    "            if key in self.progress_data:\n",
    "                self.progress_data[key]['completed'] = True\n",
    "                self._save_progress()\n",
    "    \n",
    "    def is_completed(self, source: str, brand: str = None) -> bool:\n",
    "        key = f\"{source}_{brand}\" if brand else source\n",
    "        return self.progress_data.get(key, {}).get('completed', False)\n",
    "    \n",
    "    def reset(self, source: str = None, brand: str = None):\n",
    "        with self.lock:\n",
    "            if source is None:\n",
    "                self.progress_data = {}\n",
    "            else:\n",
    "                key = f\"{source}_{brand}\" if brand else source\n",
    "                if key in self.progress_data:\n",
    "                    del self.progress_data[key]\n",
    "            self._save_progress()\n",
    "\n",
    "print(\"âœ“ ProgressTracker class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb49b13",
   "metadata": {},
   "source": [
    "## 6. Bonbanh Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0502f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonbanhCrawler:\n",
    "    \"\"\"Crawler for Bonbanh.com car listings.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://bonbanh.com/oto/\"\n",
    "    \n",
    "    def __init__(self, delay: float = 1.0):\n",
    "        self.headers = get_headers()\n",
    "        self.delay = delay\n",
    "        self.current_brand = None\n",
    "        self.csv_lock = threading.Lock()\n",
    "        self.progress = ProgressTracker()\n",
    "    \n",
    "    def crawl_car_details(self, link: str, brand: str = None) -> Optional[Dict[str, str]]:\n",
    "        \"\"\"Crawl details of a single car.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(link, headers=self.headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'lxml')\n",
    "                car_data = parse_bonbanh_car_details(soup, brand=brand or self.current_brand)\n",
    "                car_data['url'] = link\n",
    "                return car_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _append_to_csv(self, car_data: Dict, filepath: Path):\n",
    "        \"\"\"Append single car to CSV immediately.\"\"\"\n",
    "        try:\n",
    "            car_data.pop('crawl_timestamp', None)\n",
    "            car_data.pop('title', None)\n",
    "            df = pd.DataFrame([car_data])\n",
    "            \n",
    "            with self.csv_lock:\n",
    "                if filepath.exists():\n",
    "                    df.to_csv(filepath, mode='a', header=False, index=False, encoding='utf-8-sig')\n",
    "                else:\n",
    "                    df.to_csv(filepath, mode='w', header=True, index=False, encoding='utf-8-sig')\n",
    "        except Exception as e:\n",
    "            print(f\"CSV error: {e}\")\n",
    "    \n",
    "    def crawl_brand(self, brand: str, max_cars: int = 500, max_pages: int = None, save_to_csv: bool = True):\n",
    "        \"\"\"Crawl cars of a specific brand.\"\"\"\n",
    "        self.current_brand = brand\n",
    "        cars_crawled = 0\n",
    "        \n",
    "        # Setup CSV\n",
    "        output_path = Path(\"../data\")\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        csv_filepath = output_path / \"bonbanh.csv\"\n",
    "        \n",
    "        # Check progress\n",
    "        if self.progress.is_completed('bonbanh', brand):\n",
    "            print(f\"âš  Brand '{brand}' already completed\")\n",
    "            return cars_crawled\n",
    "        \n",
    "        page = self.progress.get_last_page('bonbanh', brand)\n",
    "        if page > 0:\n",
    "            print(f\"ðŸ“ Resuming from page {page + 1}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Crawling {brand} - Max: {max_cars} cars\")\n",
    "        print(f\"CSV: {'Enabled' if save_to_csv else 'Disabled'}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        while True:\n",
    "            if max_pages and page >= max_pages:\n",
    "                break\n",
    "            if max_cars and cars_crawled >= max_cars:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                url = f'{self.BASE_URL}{brand}/page,{page}'\n",
    "                print(f\"Page {page + 1}: {url}\")\n",
    "                \n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'lxml')\n",
    "                    car_links = parse_bonbanh_listing(soup)\n",
    "                    \n",
    "                    if not car_links:\n",
    "                        print(\"No more cars found\")\n",
    "                        break\n",
    "                    \n",
    "                    print(f\"Found {len(car_links)} cars\")\n",
    "                    \n",
    "                    for link in car_links:\n",
    "                        if max_cars and cars_crawled >= max_cars:\n",
    "                            break\n",
    "                        \n",
    "                        car_data = self.crawl_car_details(link, brand=brand)\n",
    "                        if car_data:\n",
    "                            if save_to_csv:\n",
    "                                self._append_to_csv(car_data, csv_filepath)\n",
    "                            cars_crawled += 1\n",
    "                            print(f\"  [{cars_crawled}] âœ“ {car_data.get('model', 'Unknown')}\")\n",
    "                        \n",
    "                        rate_limit(self.delay)\n",
    "                    \n",
    "                    self.progress.update_page('bonbanh', page, brand)\n",
    "                    page += 1\n",
    "                else:\n",
    "                    print(f\"Failed: {response.status_code}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Completed: {cars_crawled} cars\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        if not (max_cars and cars_crawled >= max_cars):\n",
    "            self.progress.mark_completed('bonbanh', brand)\n",
    "        \n",
    "        return cars_crawled\n",
    "\n",
    "print(\"âœ“ BonbanhCrawler class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426eba5",
   "metadata": {},
   "source": [
    "## 7. Chotot Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3970f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChototCrawler:\n",
    "    \"\"\"Crawler for Chotot.com car listings with multi-threaded queue-based processing.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://xe.chotot.com/mua-ban-oto\"\n",
    "    \n",
    "    def __init__(self, delay: float = 0.5, max_workers: int = 5):\n",
    "        self.headers = get_headers()\n",
    "        self.delay = delay\n",
    "        self.max_workers = max_workers\n",
    "        self.csv_lock = threading.Lock()\n",
    "        self.counter_lock = threading.Lock()\n",
    "        self.progress = ProgressTracker()\n",
    "    \n",
    "    def crawl_car_details(self, link: str) -> Optional[Dict[str, str]]:\n",
    "        \"\"\"Crawl details of a single car.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(link, headers=self.headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'lxml')\n",
    "                car_data = parse_chotot_car_details(soup)\n",
    "                car_data['url'] = link\n",
    "                return car_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {link}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _append_to_csv(self, car_data: Dict, filepath: Path):\n",
    "        \"\"\"Append single car to CSV immediately (thread-safe).\"\"\"\n",
    "        try:\n",
    "            car_data.pop('crawl_timestamp', None)\n",
    "            car_data.pop('title', None)\n",
    "            df = pd.DataFrame([car_data])\n",
    "            \n",
    "            with self.csv_lock:\n",
    "                if filepath.exists():\n",
    "                    df.to_csv(filepath, mode='a', header=False, index=False, encoding='utf-8-sig')\n",
    "                else:\n",
    "                    df.to_csv(filepath, mode='w', header=True, index=False, encoding='utf-8-sig')\n",
    "        except Exception as e:\n",
    "            print(f\"CSV error: {e}\")\n",
    "    \n",
    "    def _process_car_link(self, link: str, csv_filepath: Path, save_to_csv: bool) -> bool:\n",
    "        \"\"\"Worker function to process a single car link from queue.\"\"\"\n",
    "        car_data = self.crawl_car_details(link)\n",
    "        if car_data:\n",
    "            if save_to_csv:\n",
    "                self._append_to_csv(car_data, csv_filepath)\n",
    "            rate_limit(self.delay)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def crawl_listings(self, max_cars: int = 5000, max_pages: int = None, save_to_csv: bool = True):\n",
    "        \"\"\"Crawl car listings from Chotot using queue-based multi-threading.\"\"\"\n",
    "        cars_crawled = 0\n",
    "        \n",
    "        # Setup CSV\n",
    "        output_path = Path(\"../data\")\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        csv_filepath = output_path / \"chotot.csv\"\n",
    "        \n",
    "        # Check progress\n",
    "        if self.progress.is_completed('chotot'):\n",
    "            print(\"âš  Chotot already completed\")\n",
    "            return cars_crawled\n",
    "        \n",
    "        page = self.progress.get_last_page('chotot')\n",
    "        if page > 0:\n",
    "            page += 1\n",
    "            print(f\"ðŸ“ Resuming from page {page}\")\n",
    "        else:\n",
    "            page = 1\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Crawling Chotot.com - Max: {max_cars} cars\")\n",
    "        print(f\"Multi-threading: {self.max_workers} workers\")\n",
    "        print(f\"CSV: {'Enabled' if save_to_csv else 'Disabled'}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        while True:\n",
    "            if max_pages and page > max_pages:\n",
    "                break\n",
    "            if max_cars and cars_crawled >= max_cars:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                url = f'{self.BASE_URL}?page={page}'\n",
    "                print(f\"Page {page}: {url}\")\n",
    "                \n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'lxml')\n",
    "                    car_links = parse_chotot_listing(soup)\n",
    "                    \n",
    "                    if not car_links:\n",
    "                        print(\"No more cars found\")\n",
    "                        break\n",
    "                    \n",
    "                    print(f\"Found {len(car_links)} cars on page {page}\")\n",
    "                    \n",
    "                    # Calculate how many cars to process from this page\n",
    "                    remaining_quota = max_cars - cars_crawled if max_cars else len(car_links)\n",
    "                    links_to_process = car_links[:remaining_quota]\n",
    "                    \n",
    "                    # Process car links in parallel using ThreadPoolExecutor\n",
    "                    with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                        # Submit all car link processing tasks\n",
    "                        future_to_link = {\n",
    "                            executor.submit(self._process_car_link, link, csv_filepath, save_to_csv): link \n",
    "                            for link in links_to_process\n",
    "                        }\n",
    "                        \n",
    "                        # Collect results as they complete\n",
    "                        page_crawled = 0\n",
    "                        for future in concurrent.futures.as_completed(future_to_link):\n",
    "                            link = future_to_link[future]\n",
    "                            try:\n",
    "                                success = future.result()\n",
    "                                if success:\n",
    "                                    with self.counter_lock:\n",
    "                                        cars_crawled += 1\n",
    "                                        page_crawled += 1\n",
    "                                    print(f\"  [{cars_crawled}] âœ“ Crawled\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"  âœ— Error processing {link}: {e}\")\n",
    "                    \n",
    "                    print(f\"Page {page} completed: {page_crawled}/{len(links_to_process)} cars processed\")\n",
    "                    \n",
    "                    self.progress.update_page('chotot', page)\n",
    "                    page += 1\n",
    "                    \n",
    "                    # Check if we've reached the max_cars limit\n",
    "                    if max_cars and cars_crawled >= max_cars:\n",
    "                        break\n",
    "                else:\n",
    "                    print(f\"Failed: {response.status_code}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Completed: {cars_crawled} cars\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        if not (max_cars and cars_crawled >= max_cars):\n",
    "            self.progress.mark_completed('chotot')\n",
    "        \n",
    "        return cars_crawled\n",
    "\n",
    "print(\"âœ“ ChototCrawler class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bc5bb",
   "metadata": {},
   "source": [
    "## 8. Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f593cec",
   "metadata": {},
   "source": [
    "### Example 1: Crawl Single Brand (Bonbanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crawl Toyota cars from Bonbanh \n",
    "# bonbanh_crawler = BonbanhCrawler(delay=1.0)\n",
    "# cars_count = bonbanh_crawler.crawl_brand(\n",
    "#     brand='toyota',\n",
    "#     max_cars=10  # Change to 500 for full crawl\n",
    "# )\n",
    "\n",
    "# print(f\"\\nTotal cars crawled and saved: {cars_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42035bdf",
   "metadata": {},
   "source": [
    "### Example 2: Crawl Chotot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82974ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crawl cars from Chotot\n",
    "# chotot_crawler = ChototCrawler(delay=0.5)\n",
    "# cars_count = chotot_crawler.crawl_listings(\n",
    "#     max_cars=10  # Change to 5000 for full crawl\n",
    "# )\n",
    "\n",
    "# print(f\"\\nTotal cars crawled and saved: {cars_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e2b94",
   "metadata": {},
   "source": [
    "### Example 3: Crawl Multiple Brands with Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf26e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crawl_single_brand(brand: str, max_cars: int = 30):\n",
    "#     \"\"\"Helper function for parallel crawling.\"\"\"\n",
    "#     crawler = BonbanhCrawler(delay=0.0)\n",
    "#     return crawler.crawl_brand(brand, max_cars=max_cars)\n",
    "\n",
    "# # Crawl multiple brands in parallel \n",
    "# brands_to_crawl = CAR_BRANDS\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#     future_to_brand = {\n",
    "#         executor.submit(crawl_single_brand, brand, 5): brand \n",
    "#         for brand in brands_to_crawl\n",
    "#     }\n",
    "    \n",
    "#     total_cars = 0\n",
    "#     for future in concurrent.futures.as_completed(future_to_brand):\n",
    "#         brand = future_to_brand[future]\n",
    "#         try:\n",
    "#             count = future.result()\n",
    "#             total_cars += count\n",
    "#             print(f\"âœ“ {brand}: {count} cars saved\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"âœ— {brand}: {e}\")\n",
    "\n",
    "# print(f\"\\nTotal cars crawled: {total_cars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca09c58",
   "metadata": {},
   "source": [
    "## 9. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eaeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and explore crawled data\n",
    "# bonbanh_df = pd.read_csv('../data/bonbanh.csv')\n",
    "# chotot_df = pd.read_csv('../data/chotot.csv')\n",
    "\n",
    "# print(\"Bonbanh Data:\")\n",
    "# print(f\"Total records: {len(bonbanh_df)}\")\n",
    "# print(bonbanh_df.head())\n",
    "# print(\"\\nColumn info:\")\n",
    "# print(bonbanh_df.info())\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"Chotot Data:\")\n",
    "# print(f\"Total records: {len(chotot_df)}\")\n",
    "# print(chotot_df.head())\n",
    "# print(\"\\nColumn info:\")\n",
    "# print(chotot_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17048f71",
   "metadata": {},
   "source": [
    "## 10. Progress Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9adc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View current progress\n",
    "# progress = ProgressTracker()\n",
    "\n",
    "# # Show all progress\n",
    "# print(\"Current Progress:\")\n",
    "# print(json.dumps(progress.progress_data, indent=2, ensure_ascii=False))\n",
    "\n",
    "# # Reset specific brand\n",
    "# # progress.reset('bonbanh', 'toyota')\n",
    "\n",
    "# # Reset all progress\n",
    "# # progress.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "chotot_crawler = ChototCrawler(delay=0.0, max_workers=10)\n",
    "total_cars = chotot_crawler.crawl_listings(max_cars=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ee3aa",
   "metadata": {},
   "source": [
    "## 11. Full Production Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32aee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This will crawl ALL brands with default limits\n",
    "# Bonbanh: 500 cars per brand * 29 brands (Some brands may not have enough 500 cars)\n",
    "# Chotot: 5,000 cars\n",
    "# Multi-threading: Crawls multiple brands in parallel for faster execution\n",
    "\n",
    "# Helper function for parallel brand crawling\n",
    "def crawl_brand_worker(brand: str, max_cars: int = 500):\n",
    "    \"\"\"Worker function for parallel crawling.\"\"\"\n",
    "    crawler = BonbanhCrawler(delay=0.0)\n",
    "    return brand, crawler.crawl_brand(brand, max_cars=max_cars)\n",
    "\n",
    "# Crawl all Bonbanh brands with multi-threading\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING FULL CRAWL - {len(CAR_BRANDS)} BRANDS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "total_cars_bonbanh = 0\n",
    "max_workers = 10  # Number of parallel threads (adjust based on your system)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit all brand crawling tasks\n",
    "    future_to_brand = {\n",
    "        executor.submit(crawl_brand_worker, brand, 500): brand \n",
    "        for brand in CAR_BRANDS\n",
    "    }\n",
    "    \n",
    "    # Process completed tasks as they finish\n",
    "    for i, future in enumerate(concurrent.futures.as_completed(future_to_brand), 1):\n",
    "        brand = future_to_brand[future]\n",
    "        try:\n",
    "            brand_name, count = future.result()\n",
    "            total_cars_bonbanh += count\n",
    "            print(f\"\\n[{i}/{len(CAR_BRANDS)}] âœ“ {brand_name.upper()}: {count} cars\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[{i}/{len(CAR_BRANDS)}] âœ— {brand.upper()}: Error - {e}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BONBANH COMPLETED: {total_cars_bonbanh} total cars from {len(CAR_BRANDS)} brands\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Crawl Chotot (single-threaded as it's one source)\n",
    "chotot_crawler = ChototCrawler(delay=0.0, max_workers=10)\n",
    "total_cars_chotot = chotot_crawler.crawl_listings(max_cars=5000)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FULL CRAWL COMPLETED\")\n",
    "print(f\"  Bonbanh: {total_cars_bonbanh} cars\")\n",
    "print(f\"  Chotot: {total_cars_chotot} cars\")\n",
    "print(f\"  TOTAL: {total_cars_bonbanh + total_cars_chotot} cars\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
