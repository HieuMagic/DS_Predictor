{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a7a6b6",
   "metadata": {},
   "source": [
    "## 0. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8698bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from queue import Queue\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f65e0",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d29530d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data cleaning functions defined\n"
     ]
    }
   ],
   "source": [
    "def clean_integer(value):\n",
    "    \"\"\"\n",
    "    Convert value to integer safely.\n",
    "    If 'KhÃ¡c', garbage text, or null -> return -1\n",
    "    Examples: \"5 chá»—\" -> 5, \"7 cá»­a\" -> 7\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return -1\n",
    "    \n",
    "    val_str = str(value).strip()\n",
    "    \n",
    "    # If already a number\n",
    "    if val_str.isdigit() or (val_str.startswith('-') and val_str[1:].isdigit()):\n",
    "        return int(val_str)\n",
    "        \n",
    "    # Handle cases like \"5 chá»—\", \"5 cá»­a\" -> Extract number 5\n",
    "    # Use regex to keep only digits\n",
    "    nums = re.findall(r'\\d+', val_str)\n",
    "    if nums:\n",
    "        return int(nums[0])\n",
    "        \n",
    "    # Other cases (e.g., 'KhÃ¡c') -> -1\n",
    "    return -1\n",
    "\n",
    "def clean_float(value):\n",
    "    \"\"\"\n",
    "    Convert to float (for engine capacity).\n",
    "    Examples: \"2.0 L\" -> 2.0, \"1.5\" -> 1.5\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return -1.0\n",
    "    try:\n",
    "        # Handle \"2.0 L\" -> 2.0\n",
    "        val_str = str(value).upper().replace('L', '').replace(',', '').strip()\n",
    "        return float(val_str)\n",
    "    except:\n",
    "        return -1.0\n",
    "\n",
    "def clean_string(value):\n",
    "    \"\"\"\n",
    "    Clean string values, convert -1 or empty to -1\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return -1\n",
    "    \n",
    "    val_str = str(value).strip()\n",
    "    \n",
    "    # If empty or \"-1\" string\n",
    "    if val_str == '' or val_str == '-1':\n",
    "        return -1\n",
    "        \n",
    "    return val_str\n",
    "\n",
    "def clean_car_data(car_data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Clean and format car data before saving to CSV.\n",
    "    Applies all cleaning functions to ensure data consistency.\n",
    "    \"\"\"\n",
    "    cleaned = {\n",
    "        'url': str(car_data.get('url', -1)),\n",
    "        'price': clean_integer(car_data.get('price', -1)),\n",
    "        'brand': clean_string(car_data.get('brand', -1)),\n",
    "        'model': clean_string(car_data.get('model', -1)),\n",
    "        'year': clean_integer(car_data.get('year', -1)),\n",
    "        'odometer': clean_integer(car_data.get('odometer', -1)),\n",
    "        'transmission': clean_string(car_data.get('transmission', -1)),\n",
    "        'fuel_type': clean_string(car_data.get('fuel_type', -1)),\n",
    "        'engine_capacity': clean_float(car_data.get('engine_capacity', -1)),\n",
    "        'body_style': clean_string(car_data.get('body_style', -1)),\n",
    "        'origin': clean_string(car_data.get('origin', -1)),\n",
    "        'seats': clean_integer(car_data.get('seats', -1)),\n",
    "        'condition': clean_string(car_data.get('condition', -1)),\n",
    "        'num_owners': clean_integer(car_data.get('num_owners', -1)),\n",
    "        'inspection_status': clean_integer(car_data.get('inspection_status', -1)),\n",
    "        'warranty_status': clean_integer(car_data.get('warranty_status', -1)),\n",
    "        'source': clean_string(car_data.get('source', -1))\n",
    "    }\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(\"âœ“ Data cleaning functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9721c2",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52aa424b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Total brands available: 29\n"
     ]
    }
   ],
   "source": [
    "# HTTP Headers\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Car brands list (113 brands)\n",
    "CAR_BRANDS = [\n",
    "    \"toyota\", \"hyundai\", \"kia\", \"ford\", \"honda\", \n",
    "    \"mazda\", \"mitsubishi\", \"vinfast\", \"chevrolet\", \n",
    "    \"nissan\", \"suzuki\",\n",
    "    \n",
    "    \"mercedes_benz\", \"bmw\", \"audi\", \"lexus\", \n",
    "    \"landrover\", \"porsche\", \"volvo\", \"peugeot\", \n",
    "    \"mini\", \"subaru\", \"volkswagen\",\n",
    "    \n",
    "    \"daewoo\", \n",
    "\n",
    "    \"isuzu\", \"jeep\",\n",
    "\n",
    "    \"mg\", \"baic\", \"wuling\", \"byd\" \n",
    "]\n",
    "\n",
    "# Brand mapping (URL code -> Display name)\n",
    "BRAND_MAPPING = {\n",
    "    \"toyota\": \"Toyota\",\n",
    "    \"honda\": \"Honda\",\n",
    "    \"mazda\": \"Mazda\",\n",
    "    \"mitsubishi\": \"Mitsubishi\",\n",
    "    \"nissan\": \"Nissan\",\n",
    "    \"suzuki\": \"Suzuki\",\n",
    "    \"subaru\": \"Subaru\",\n",
    "    \"isuzu\": \"Isuzu\",\n",
    "    \"lexus\": \"Lexus\",\n",
    "    \n",
    "    \"hyundai\": \"Hyundai\",\n",
    "    \"kia\": \"Kia\",\n",
    "    \"daewoo\": \"Daewoo\",\n",
    "    \"ssangyong\": \"Ssangyong\",\n",
    "    \n",
    "    \"ford\": \"Ford\",\n",
    "    \"chevrolet\": \"Chevrolet\",\n",
    "    \"jeep\": \"Jeep\",\n",
    "    \n",
    "    \"mercedes_benz\": \"Mercedes-Benz\",\n",
    "    \"bmw\": \"BMW\",\n",
    "    \"audi\": \"Audi\",\n",
    "    \"volkswagen\": \"Volkswagen\",\n",
    "    \"porsche\": \"Porsche\",\n",
    "    \"peugeot\": \"Peugeot\",\n",
    "    \"landrover\": \"Land Rover\",\n",
    "    \"volvo\": \"Volvo\",\n",
    "    \"mini\": \"MINI\",\n",
    "    \n",
    "    \"vinfast\": \"VinFast\",\n",
    "    \n",
    "    \"mg\": \"MG\",\n",
    "    \"baic\": \"BAIC\",\n",
    "    \"byd\": \"BYD\",\n",
    "    \"wuling\": \"Wuling\"\n",
    "}\n",
    "\n",
    "print(f\"âœ“ Total brands available: {len(CAR_BRANDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ad7f8",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8a6e523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def rate_limit(delay: float = 0.0):\n",
    "    \"\"\"Add delay between requests.\"\"\"\n",
    "    time.sleep(delay)\n",
    "\n",
    "def get_headers() -> dict:\n",
    "    \"\"\"Get HTTP headers for requests.\"\"\"\n",
    "    return HEADERS.copy()\n",
    "\n",
    "def split_price(title: str) -> int:\n",
    "    \"\"\"\n",
    "    Extract price from Vietnamese title with Tá»· and Triá»‡u.\n",
    "    Example: '2 Tá»· 500 Triá»‡u' -> 2500000000 VND\n",
    "    \"\"\"\n",
    "    match_ty = re.search(r'(\\d+)\\s*Tá»·', title, re.IGNORECASE)\n",
    "    ty = int(match_ty.group(1)) if match_ty else 0\n",
    "    \n",
    "    match_trieu = re.search(r'(\\d+)\\s*Triá»‡u', title, re.IGNORECASE)\n",
    "    trieu = int(match_trieu.group(1)) if match_trieu else 0\n",
    "    \n",
    "    price = (ty * 1_000_000_000) + (trieu * 1_000_000)\n",
    "    return price\n",
    "\n",
    "def extract_model_from_title(title: str, brand: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract car model and year from title.\n",
    "    Returns: (model, year)\n",
    "    \"\"\"\n",
    "    # Remove price information\n",
    "    price_pattern = r'\\d+\\s*Tá»·|\\d+\\s*Triá»‡u'\n",
    "    price_match = re.search(price_pattern, title)\n",
    "    title_without_price = title[:price_match.start()].strip() if price_match else title\n",
    "    \n",
    "    # Remove brand name\n",
    "    brand_display = BRAND_MAPPING.get(brand, brand).replace('_', ' ')\n",
    "    title_lower = title_without_price.lower()\n",
    "    brand_lower = brand_display.lower()\n",
    "    \n",
    "    if title_lower.startswith(brand_lower):\n",
    "        remaining = title_without_price[len(brand_display):].strip()\n",
    "    else:\n",
    "        remaining = title_without_price\n",
    "    \n",
    "    # Extract year\n",
    "    year_pattern = r'\\b(19|20)\\d{2}\\b'\n",
    "    year_match = re.search(year_pattern, remaining)\n",
    "    \n",
    "    if year_match:\n",
    "        model = remaining[:year_match.start()].strip()\n",
    "        year = year_match.group()\n",
    "    else:\n",
    "        words = remaining.split()\n",
    "        model = ' '.join(words[:3]) if len(words) >= 3 else remaining\n",
    "        year = None\n",
    "    \n",
    "    return model, year\n",
    "\n",
    "def extract_engine_info(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract fuel type and engine capacity.\n",
    "    Example: 'XÄƒng 2.0 L' -> ('XÄƒng', '2.0')\n",
    "    \"\"\"\n",
    "    if \":\" in text:\n",
    "        value_part = text.split(\":\", 1)[1].strip()\n",
    "    else:\n",
    "        value_part = text.strip()\n",
    "    \n",
    "    match = re.search(r'^(.*?)\\s+(\\d+(?:\\.\\d+)?)\\s*L$', value_part, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip(), match.group(2).strip()\n",
    "    else:\n",
    "        return value_part, None\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5abe42",
   "metadata": {},
   "source": [
    "## 4. Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccd1a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Parsing functions defined\n"
     ]
    }
   ],
   "source": [
    "def parse_bonbanh_car_details(soup: BeautifulSoup, brand: str = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse car details from Bonbanh.com page.\n",
    "    Returns structured dictionary with 15 fields.\n",
    "    \"\"\"\n",
    "    car_data = {\n",
    "        'price': -1, 'brand': -1, 'model': -1, 'year': -1, 'odometer': -1,\n",
    "        'transmission': -1, 'fuel_type': -1, 'engine_capacity': -1,\n",
    "        'body_style': -1, 'origin': -1, 'seats': -1, 'condition': -1,\n",
    "        'num_owners': -1, 'inspection_status': -1, 'warranty_status': -1,\n",
    "        'source': 'bonbanh'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Extract title and price\n",
    "        title_div = soup.find('div', class_='title')\n",
    "        if title_div and title_div.find('h1'):\n",
    "            title = title_div.find('h1').text\n",
    "            title = re.sub(r'\\s+', ' ', title).strip()\n",
    "            \n",
    "            price = split_price(title)\n",
    "            car_data['price'] = price if price else -1\n",
    "            \n",
    "            if brand:\n",
    "                car_data['brand'] = BRAND_MAPPING.get(brand, brand)\n",
    "                model, year = extract_model_from_title(title, brand)\n",
    "                car_data['model'] = model if model else -1\n",
    "                car_data['year'] = year if year else -1\n",
    "        \n",
    "        # Extract details\n",
    "        detail_div = soup.find('div', class_='box_car_detail')\n",
    "        if detail_div:\n",
    "            mail_parents = detail_div.find_all('div', id='mail_parent')\n",
    "            \n",
    "            for mail_parent in mail_parents:\n",
    "                label_elem = mail_parent.find('label')\n",
    "                span_elem = mail_parent.find('span')\n",
    "                \n",
    "                if not label_elem or not span_elem:\n",
    "                    continue\n",
    "                \n",
    "                label = re.sub(r'\\s+', ' ', label_elem.text).strip().rstrip(':')\n",
    "                value = re.sub(r'\\s+', ' ', span_elem.text).strip()\n",
    "                \n",
    "                # Map labels to fields\n",
    "                if label == 'Sá»‘ Km Ä‘Ã£ Ä‘i':\n",
    "                    car_data['odometer'] = re.sub(r'\\D', '', value)\n",
    "                elif label == 'Há»™p sá»‘':\n",
    "                    car_data['transmission'] = value\n",
    "                elif label == 'Äá»™ng cÆ¡':\n",
    "                    fuel_type, engine_capacity = extract_engine_info(value)\n",
    "                    car_data['fuel_type'] = fuel_type if fuel_type else -1\n",
    "                    car_data['engine_capacity'] = engine_capacity if engine_capacity else -1\n",
    "                elif label == 'Kiá»ƒu dÃ¡ng':\n",
    "                    car_data['body_style'] = value\n",
    "                elif label == 'Xuáº¥t xá»©':\n",
    "                    car_data['origin'] = value\n",
    "                elif label == 'Sá»‘ chá»— ngá»“i':\n",
    "                    car_data['seats'] = value\n",
    "                elif label == 'TÃ¬nh tráº¡ng':\n",
    "                    car_data['condition'] = value\n",
    "                elif 'chá»§' in label.lower():\n",
    "                    car_data['num_owners'] = value\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {e}\")\n",
    "    \n",
    "    return car_data\n",
    "\n",
    "def parse_bonbanh_listing(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"Extract car detail URLs from Bonbanh listing page.\"\"\"\n",
    "    car_links = []\n",
    "    try:\n",
    "        car_lis = soup.find_all('li', class_='car-item')\n",
    "        for car_li in car_lis:\n",
    "            a_link = car_li.find('a')\n",
    "            if a_link and 'href' in a_link.attrs:\n",
    "                link = 'https://bonbanh.com/' + a_link.attrs['href']\n",
    "                car_links.append(link)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing listing: {e}\")\n",
    "    return car_links\n",
    "\n",
    "def parse_chotot_car_details(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"Parse car details from Chotot.com page.\"\"\"\n",
    "    car_data = {\n",
    "        'price': -1, 'brand': -1, 'model': -1, 'year': -1, 'odometer': -1,\n",
    "        'transmission': -1, 'fuel_type': -1, 'engine_capacity': -1,\n",
    "        'body_style': -1, 'origin': -1, 'seats': -1, 'condition': -1,\n",
    "        'num_owners': -1, 'inspection_status': -1, 'warranty_status': -1,\n",
    "        'source': 'chotot'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Extract price\n",
    "        div_title = soup.find('div', class_='cpmughi')\n",
    "        if div_title:\n",
    "            price_elem = div_title.find('b', class_='p26z2wb')\n",
    "            if price_elem:\n",
    "                price = re.sub(r'\\D', '', price_elem.text.strip())\n",
    "                car_data['price'] = price if price else -1\n",
    "        \n",
    "        # Extract other details\n",
    "        div_list = soup.find_all('div', class_='p1ja3eq0')\n",
    "        for div in div_list:\n",
    "            label_elem = div.find('span', class_='bwq0cbs')\n",
    "            if label_elem:\n",
    "                value_elem = label_elem.find_next_sibling('span')\n",
    "                if value_elem:\n",
    "                    label = label_elem.text.strip()\n",
    "                    value = value_elem.text.strip()\n",
    "                    \n",
    "                    # Map labels\n",
    "                    if label == 'Sá»‘ Km Ä‘Ã£ Ä‘i': car_data['odometer'] = value\n",
    "                    elif label == 'Há»™p sá»‘': car_data['transmission'] = value\n",
    "                    elif label == 'NhiÃªn liá»‡u': car_data['fuel_type'] = value\n",
    "                    elif label == 'Kiá»ƒu dÃ¡ng': car_data['body_style'] = value\n",
    "                    elif label == 'Xuáº¥t xá»©': car_data['origin'] = value\n",
    "                    elif label == 'Sá»‘ chá»—': car_data['seats'] = value\n",
    "                    elif label == 'TÃ¬nh tráº¡ng': car_data['condition'] = value\n",
    "                    elif label == 'HÃ£ng': car_data['brand'] = value\n",
    "                    elif label == 'NÄƒm sáº£n xuáº¥t': car_data['year'] = value\n",
    "                    elif label == 'DÃ²ng xe': car_data['model'] = value\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {e}\")\n",
    "    \n",
    "    return car_data\n",
    "\n",
    "def parse_chotot_listing(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"Extract car detail URLs from Chotot listing page.\"\"\"\n",
    "    car_links = []\n",
    "    base_url = 'https://xe.chotot.com'\n",
    "    try:\n",
    "        a_list = soup.find_all('a', class_='c15fd2pn')\n",
    "        for a in a_list:\n",
    "            if 'href' in a.attrs:\n",
    "                link = a.attrs['href']\n",
    "                if not link.startswith('http'):\n",
    "                    link = base_url + link\n",
    "                car_links.append(link)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing listing: {e}\")\n",
    "    return car_links\n",
    "\n",
    "print(\"âœ“ Parsing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066974a",
   "metadata": {},
   "source": [
    "## 5. Progress Tracker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d85f9db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ProgressTracker class defined\n"
     ]
    }
   ],
   "source": [
    "class ProgressTracker:\n",
    "    \"\"\"Track crawling progress to enable resume functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, progress_file: str = \"../data/crawl_progress.json\"):\n",
    "        self.progress_file = Path(progress_file)\n",
    "        self.progress_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.lock = threading.Lock()\n",
    "        self.progress_data = self._load_progress()\n",
    "    \n",
    "    def _load_progress(self) -> Dict:\n",
    "        if self.progress_file.exists():\n",
    "            try:\n",
    "                with open(self.progress_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def _save_progress(self):\n",
    "        try:\n",
    "            with open(self.progress_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.progress_data, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save progress: {e}\")\n",
    "    \n",
    "    def get_last_page(self, source: str, brand: str = None) -> int:\n",
    "        key = f\"{source}_{brand}\" if brand else source\n",
    "        return self.progress_data.get(key, {}).get('last_page', 0)\n",
    "    \n",
    "    def update_page(self, source: str, page: int, brand: str = None):\n",
    "        with self.lock:\n",
    "            key = f\"{source}_{brand}\" if brand else source\n",
    "            if key not in self.progress_data:\n",
    "                self.progress_data[key] = {}\n",
    "            self.progress_data[key]['last_page'] = page\n",
    "            self._save_progress()\n",
    "    \n",
    "    def mark_completed(self, source: str, brand: str = None):\n",
    "        with self.lock:\n",
    "            key = f\"{source}_{brand}\" if brand else source\n",
    "            if key in self.progress_data:\n",
    "                self.progress_data[key]['completed'] = True\n",
    "                self._save_progress()\n",
    "    \n",
    "    def is_completed(self, source: str, brand: str = None) -> bool:\n",
    "        key = f\"{source}_{brand}\" if brand else source\n",
    "        return self.progress_data.get(key, {}).get('completed', False)\n",
    "    \n",
    "    def reset(self, source: str = None, brand: str = None):\n",
    "        with self.lock:\n",
    "            if source is None:\n",
    "                self.progress_data = {}\n",
    "            else:\n",
    "                key = f\"{source}_{brand}\" if brand else source\n",
    "                if key in self.progress_data:\n",
    "                    del self.progress_data[key]\n",
    "            self._save_progress()\n",
    "\n",
    "print(\"âœ“ ProgressTracker class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb49b13",
   "metadata": {},
   "source": [
    "## 6. Bonbanh Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf0502f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ BonbanhCrawler class defined\n"
     ]
    }
   ],
   "source": [
    "class BonbanhCrawler:\n",
    "    \"\"\"Crawler for Bonbanh.com car listings.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://bonbanh.com/oto/\"\n",
    "    \n",
    "    def __init__(self, delay: float = 1.0):\n",
    "        self.headers = get_headers()\n",
    "        self.delay = delay\n",
    "        self.current_brand = None\n",
    "        self.csv_lock = threading.Lock()\n",
    "        self.progress = ProgressTracker()\n",
    "        self.existing_urls = set()  # Track URLs to prevent duplicates\n",
    "    \n",
    "    def crawl_car_details(self, link: str, brand: str = None) -> Optional[Dict[str, str]]:\n",
    "        \"\"\"Crawl details of a single car.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(link, headers=self.headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'lxml')\n",
    "                car_data = parse_bonbanh_car_details(soup, brand=brand or self.current_brand)\n",
    "                car_data['url'] = link\n",
    "                return car_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _append_to_csv(self, car_data: Dict, filepath: Path) -> bool:\n",
    "        \"\"\"Append single car to CSV immediately with data cleaning. Returns True if added, False if duplicate.\"\"\"\n",
    "        try:\n",
    "            car_url = car_data.get('url')\n",
    "            \n",
    "            # Check for duplicate URL\n",
    "            with self.csv_lock:\n",
    "                if car_url in self.existing_urls:\n",
    "                    return False  # Skip duplicate\n",
    "                \n",
    "                car_data.pop('crawl_timestamp', None)\n",
    "                car_data.pop('title', None)\n",
    "                \n",
    "                # Clean and format data before saving\n",
    "                cleaned_data = clean_car_data(car_data)\n",
    "                df = pd.DataFrame([cleaned_data])\n",
    "                \n",
    "                if filepath.exists():\n",
    "                    df.to_csv(filepath, mode='a', header=False, index=False, encoding='utf-8-sig')\n",
    "                else:\n",
    "                    df.to_csv(filepath, mode='w', header=True, index=False, encoding='utf-8-sig')\n",
    "                \n",
    "                # Add URL to set after successful write\n",
    "                self.existing_urls.add(car_url)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"CSV error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def crawl_brand(self, brand: str, max_cars: int = 500, max_pages: int = None, save_to_csv: bool = True):\n",
    "        \"\"\"Crawl cars of a specific brand.\"\"\"\n",
    "        self.current_brand = brand\n",
    "        cars_crawled = 0\n",
    "        \n",
    "        # Setup CSV\n",
    "        output_path = Path(\"../data\")\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        csv_filepath = output_path / \"car_data.csv\"\n",
    "        \n",
    "        # Load existing URLs to prevent duplicates\n",
    "        if csv_filepath.exists():\n",
    "            try:\n",
    "                existing_df = pd.read_csv(csv_filepath)\n",
    "                self.existing_urls = set(existing_df['url'].dropna().tolist())\n",
    "                print(f\"ðŸ“‹ Loaded {len(self.existing_urls)} existing URLs\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Could not load existing URLs: {e}\")\n",
    "                self.existing_urls = set()\n",
    "        \n",
    "        # Check progress\n",
    "        if self.progress.is_completed('bonbanh', brand):\n",
    "            print(f\"âš  Brand '{brand}' already completed\")\n",
    "            return cars_crawled\n",
    "        \n",
    "        page = self.progress.get_last_page('bonbanh', brand)\n",
    "        if page > 0:\n",
    "            print(f\"ðŸ“ Resuming from page {page + 1}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Crawling {brand} - Max: {max_cars} cars\")\n",
    "        print(f\"CSV: {'Enabled' if save_to_csv else 'Disabled'}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        while True:\n",
    "            if max_pages and page >= max_pages:\n",
    "                break\n",
    "            if max_cars and cars_crawled >= max_cars:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                url = f'{self.BASE_URL}{brand}/page,{page}'\n",
    "                print(f\"Page {page + 1}: {url}\")\n",
    "                \n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'lxml')\n",
    "                    car_links = parse_bonbanh_listing(soup)\n",
    "                    \n",
    "                    if not car_links:\n",
    "                        print(\"No more cars found\")\n",
    "                        break\n",
    "                    \n",
    "                    print(f\"Found {len(car_links)} cars\")\n",
    "                    \n",
    "                    for link in car_links:\n",
    "                        if max_cars and cars_crawled >= max_cars:\n",
    "                            break\n",
    "                        \n",
    "                        car_data = self.crawl_car_details(link, brand=brand)\n",
    "                        if car_data:\n",
    "                            if save_to_csv:\n",
    "                                if self._append_to_csv(car_data, csv_filepath):\n",
    "                                    cars_crawled += 1\n",
    "                                    print(f\"  [{cars_crawled}] âœ“ {car_data.get('model', 'Unknown')}\")\n",
    "                                else:\n",
    "                                    print(f\"  [SKIP] Duplicate URL\")\n",
    "                            else:\n",
    "                                cars_crawled += 1\n",
    "                                print(f\"  [{cars_crawled}] âœ“ {car_data.get('model', 'Unknown')}\")\n",
    "                        \n",
    "                        rate_limit(self.delay)\n",
    "                    \n",
    "                    self.progress.update_page('bonbanh', page, brand)\n",
    "                    page += 1\n",
    "                else:\n",
    "                    print(f\"Failed: {response.status_code}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Completed: {cars_crawled} cars\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        if not (max_cars and cars_crawled >= max_cars):\n",
    "            self.progress.mark_completed('bonbanh', brand)\n",
    "        \n",
    "        return cars_crawled\n",
    "\n",
    "print(\"âœ“ BonbanhCrawler class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426eba5",
   "metadata": {},
   "source": [
    "## 7. Chotot Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3970f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ChototCrawler class defined\n"
     ]
    }
   ],
   "source": [
    "class ChototCrawler:\n",
    "    \"\"\"Crawler for Chotot.com car listings with multi-threaded queue-based processing.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://xe.chotot.com/mua-ban-oto\"\n",
    "    \n",
    "    def __init__(self, delay: float = 0.5, max_workers: int = 5):\n",
    "        self.headers = get_headers()\n",
    "        self.delay = delay\n",
    "        self.max_workers = max_workers\n",
    "        self.csv_lock = threading.Lock()\n",
    "        self.counter_lock = threading.Lock()\n",
    "        self.progress = ProgressTracker()\n",
    "        self.existing_urls = set()  # Track URLs to prevent duplicates\n",
    "    \n",
    "    def crawl_car_details(self, link: str) -> Optional[Dict[str, str]]:\n",
    "        \"\"\"Crawl details of a single car.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(link, headers=self.headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'lxml')\n",
    "                car_data = parse_chotot_car_details(soup)\n",
    "                car_data['url'] = link\n",
    "                return car_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {link}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def _append_to_csv(self, car_data: Dict, filepath: Path) -> bool:\n",
    "        \"\"\"Append single car to CSV immediately (thread-safe) with data cleaning. Returns True if added, False if duplicate.\"\"\"\n",
    "        try:\n",
    "            car_url = car_data.get('url')\n",
    "            \n",
    "            # Check for duplicate URL\n",
    "            with self.csv_lock:\n",
    "                if car_url in self.existing_urls:\n",
    "                    return False  # Skip duplicate\n",
    "                \n",
    "                car_data.pop('crawl_timestamp', None)\n",
    "                car_data.pop('title', None)\n",
    "                \n",
    "                # Clean and format data before saving\n",
    "                cleaned_data = clean_car_data(car_data)\n",
    "                df = pd.DataFrame([cleaned_data])\n",
    "                \n",
    "                if filepath.exists():\n",
    "                    df.to_csv(filepath, mode='a', header=False, index=False, encoding='utf-8-sig')\n",
    "                else:\n",
    "                    df.to_csv(filepath, mode='w', header=True, index=False, encoding='utf-8-sig')\n",
    "                \n",
    "                # Add URL to set after successful write\n",
    "                self.existing_urls.add(car_url)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"CSV error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _process_car_link(self, link: str, csv_filepath: Path, save_to_csv: bool) -> bool:\n",
    "        \"\"\"Worker function to process a single car link from queue.\"\"\"\n",
    "        car_data = self.crawl_car_details(link)\n",
    "        if car_data:\n",
    "            if save_to_csv:\n",
    "                return self._append_to_csv(car_data, csv_filepath)\n",
    "            rate_limit(self.delay)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def crawl_listings(self, max_cars: int = 5000, max_pages: int = None, save_to_csv: bool = True):\n",
    "        \"\"\"Crawl car listings from Chotot using queue-based multi-threading.\"\"\"\n",
    "        cars_crawled = 0\n",
    "        \n",
    "        # Setup CSV\n",
    "        output_path = Path(\"../data\")\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        csv_filepath = output_path / \"car_data.csv\"\n",
    "        \n",
    "        # Load existing URLs to prevent duplicates\n",
    "        if csv_filepath.exists():\n",
    "            try:\n",
    "                existing_df = pd.read_csv(csv_filepath)\n",
    "                self.existing_urls = set(existing_df['url'].dropna().tolist())\n",
    "                print(f\"ðŸ“‹ Loaded {len(self.existing_urls)} existing URLs\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Could not load existing URLs: {e}\")\n",
    "                self.existing_urls = set()\n",
    "        \n",
    "        # Check progress\n",
    "        if self.progress.is_completed('chotot'):\n",
    "            print(\"âš  Chotot already completed\")\n",
    "            return cars_crawled\n",
    "        \n",
    "        page = self.progress.get_last_page('chotot')\n",
    "        if page > 0:\n",
    "            page += 1\n",
    "            print(f\"ðŸ“ Resuming from page {page}\")\n",
    "        else:\n",
    "            page = 1\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Crawling Chotot.com - Max: {max_cars} cars\")\n",
    "        print(f\"Multi-threading: {self.max_workers} workers\")\n",
    "        print(f\"CSV: {'Enabled' if save_to_csv else 'Disabled'}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        while True:\n",
    "            if max_pages and page > max_pages:\n",
    "                break\n",
    "            if max_cars and cars_crawled >= max_cars:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                url = f'{self.BASE_URL}?page={page}'\n",
    "                print(f\"Page {page}: {url}\")\n",
    "                \n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'lxml')\n",
    "                    car_links = parse_chotot_listing(soup)\n",
    "                    \n",
    "                    if not car_links:\n",
    "                        print(\"No more cars found\")\n",
    "                        break\n",
    "                    \n",
    "                    print(f\"Found {len(car_links)} cars on page {page}\")\n",
    "                    \n",
    "                    # Calculate how many cars to process from this page\n",
    "                    remaining_quota = max_cars - cars_crawled if max_cars else len(car_links)\n",
    "                    links_to_process = car_links[:remaining_quota]\n",
    "                    \n",
    "                    # Process car links in parallel using ThreadPoolExecutor\n",
    "                    with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                        # Submit all car link processing tasks\n",
    "                        future_to_link = {\n",
    "                            executor.submit(self._process_car_link, link, csv_filepath, save_to_csv): link \n",
    "                            for link in links_to_process\n",
    "                        }\n",
    "                        \n",
    "                        # Collect results as they complete\n",
    "                        page_crawled = 0\n",
    "                        for future in concurrent.futures.as_completed(future_to_link):\n",
    "                            link = future_to_link[future]\n",
    "                            try:\n",
    "                                success = future.result()\n",
    "                                if success:\n",
    "                                    with self.counter_lock:\n",
    "                                        cars_crawled += 1\n",
    "                                        page_crawled += 1\n",
    "                                    print(f\"  [{cars_crawled}] âœ“ Crawled\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"  âœ— Error processing {link}: {e}\")\n",
    "                    \n",
    "                    print(f\"Page {page} completed: {page_crawled}/{len(links_to_process)} cars processed\")\n",
    "                    \n",
    "                    self.progress.update_page('chotot', page)\n",
    "                    page += 1\n",
    "                    \n",
    "                    # Check if we've reached the max_cars limit\n",
    "                    if max_cars and cars_crawled >= max_cars:\n",
    "                        break\n",
    "                else:\n",
    "                    print(f\"Failed: {response.status_code}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Completed: {cars_crawled} cars\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        if not (max_cars and cars_crawled >= max_cars):\n",
    "            self.progress.mark_completed('chotot')\n",
    "        \n",
    "        return cars_crawled\n",
    "\n",
    "print(\"âœ“ ChototCrawler class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bc5bb",
   "metadata": {},
   "source": [
    "## 8. Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f593cec",
   "metadata": {},
   "source": [
    "### Example 1: Crawl Single Brand (Bonbanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "262c9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crawl Toyota cars from Bonbanh \n",
    "# bonbanh_crawler = BonbanhCrawler(delay=1.0)\n",
    "# cars_count = bonbanh_crawler.crawl_brand(\n",
    "#     brand='toyota',\n",
    "#     max_cars=10  # Change to 500 for full crawl\n",
    "# )\n",
    "\n",
    "# print(f\"\\nTotal cars crawled and saved: {cars_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42035bdf",
   "metadata": {},
   "source": [
    "### Example 2: Crawl Chotot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82974ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crawl cars from Chotot\n",
    "# chotot_crawler = ChototCrawler(delay=0.5)\n",
    "# cars_count = chotot_crawler.crawl_listings(\n",
    "#     max_cars=10  # Change to 5000 for full crawl\n",
    "# )\n",
    "\n",
    "# print(f\"\\nTotal cars crawled and saved: {cars_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e2b94",
   "metadata": {},
   "source": [
    "### Example 3: Crawl Multiple Brands with Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf26e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crawl_single_brand(brand: str, max_cars: int = 30):\n",
    "#     \"\"\"Helper function for parallel crawling.\"\"\"\n",
    "#     crawler = BonbanhCrawler(delay=0.0)\n",
    "#     return crawler.crawl_brand(brand, max_cars=max_cars)\n",
    "\n",
    "# # Crawl multiple brands in parallel \n",
    "# brands_to_crawl = CAR_BRANDS\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#     future_to_brand = {\n",
    "#         executor.submit(crawl_single_brand, brand, 5): brand \n",
    "#         for brand in brands_to_crawl\n",
    "#     }\n",
    "    \n",
    "#     total_cars = 0\n",
    "#     for future in concurrent.futures.as_completed(future_to_brand):\n",
    "#         brand = future_to_brand[future]\n",
    "#         try:\n",
    "#             count = future.result()\n",
    "#             total_cars += count\n",
    "#             print(f\"âœ“ {brand}: {count} cars saved\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"âœ— {brand}: {e}\")\n",
    "\n",
    "# print(f\"\\nTotal cars crawled: {total_cars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca09c58",
   "metadata": {},
   "source": [
    "## 9. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93eaeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and explore crawled data\n",
    "# bonbanh_df = pd.read_csv('../data/bonbanh.csv')\n",
    "# chotot_df = pd.read_csv('../data/chotot.csv')\n",
    "\n",
    "# print(\"Bonbanh Data:\")\n",
    "# print(f\"Total records: {len(bonbanh_df)}\")\n",
    "# print(bonbanh_df.head())\n",
    "# print(\"\\nColumn info:\")\n",
    "# print(bonbanh_df.info())\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"Chotot Data:\")\n",
    "# print(f\"Total records: {len(chotot_df)}\")\n",
    "# print(chotot_df.head())\n",
    "# print(\"\\nColumn info:\")\n",
    "# print(chotot_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17048f71",
   "metadata": {},
   "source": [
    "## 10. Progress Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed9adc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View current progress\n",
    "# progress = ProgressTracker()\n",
    "\n",
    "# # Show all progress\n",
    "# print(\"Current Progress:\")\n",
    "# print(json.dumps(progress.progress_data, indent=2, ensure_ascii=False))\n",
    "\n",
    "# # Reset specific brand\n",
    "# # progress.reset('bonbanh', 'toyota')\n",
    "\n",
    "# # Reset all progress\n",
    "# # progress.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ee3aa",
   "metadata": {},
   "source": [
    "## 11. Full Production Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d32aee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING FULL CRAWL - 29 BRANDS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Crawling toyota - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/toyota/page,0\n",
      "\n",
      "============================================================\n",
      "Crawling hyundai - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/hyundai/page,0\n",
      "\n",
      "============================================================\n",
      "Crawling kia - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/kia/page,0\n",
      "\n",
      "============================================================\n",
      "Crawling ford - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/ford/page,0\n",
      "\n",
      "============================================================\n",
      "Crawling honda - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/honda/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/29] âœ“ HYUNDAI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mazda - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mazda/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[2/29] âœ“ HONDA: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mitsubishi - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mitsubishi/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[3/29] âœ“ TOYOTA: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[4/29] âœ“ KIA: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Crawling vinfast - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/vinfast/page,0\n",
      "\n",
      "[5/29] âœ“ FORD: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling chevrolet - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/chevrolet/page,0\n",
      "\n",
      "============================================================\n",
      "Crawling nissan - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/nissan/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/29] âœ“ HYUNDAI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mazda - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mazda/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[2/29] âœ“ HONDA: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mitsubishi - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mitsubishi/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[3/29] âœ“ TOYOTA: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[4/29] âœ“ KIA: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Crawling vinfast - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/vinfast/page,0\n",
      "\n",
      "[5/29] âœ“ FORD: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling chevrolet - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/chevrolet/page,0\n",
      "\n",
      "============================================================\n",
      "Crawling nissan - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/nissan/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[6/29] âœ“ NISSAN: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling suzuki - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/suzuki/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[7/29] âœ“ VINFAST: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mercedes_benz - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mercedes_benz/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[8/29] âœ“ CHEVROLET: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling bmw - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/bmw/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[9/29] âœ“ MITSUBISHI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling audi - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/audi/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[10/29] âœ“ MAZDA: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling lexus - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/lexus/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[6/29] âœ“ NISSAN: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling suzuki - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/suzuki/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[7/29] âœ“ VINFAST: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mercedes_benz - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mercedes_benz/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[8/29] âœ“ CHEVROLET: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling bmw - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/bmw/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[9/29] âœ“ MITSUBISHI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling audi - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/audi/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[10/29] âœ“ MAZDA: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling lexus - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/lexus/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[11/29] âœ“ SUZUKI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling landrover - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/landrover/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[12/29] âœ“ MERCEDES_BENZ: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling porsche - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/porsche/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[13/29] âœ“ BMW: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling volvo - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/volvo/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[14/29] âœ“ AUDI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling peugeot - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/peugeot/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[15/29] âœ“ LEXUS: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mini - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mini/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[11/29] âœ“ SUZUKI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling landrover - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/landrover/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[12/29] âœ“ MERCEDES_BENZ: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling porsche - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/porsche/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[13/29] âœ“ BMW: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling volvo - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/volvo/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[14/29] âœ“ AUDI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling peugeot - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/peugeot/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[15/29] âœ“ LEXUS: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mini - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mini/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[16/29] âœ“ LANDROVER: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling subaru - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/subaru/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[17/29] âœ“ PORSCHE: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling volkswagen - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/volkswagen/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[18/29] âœ“ VOLVO: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling daewoo - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/daewoo/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[19/29] âœ“ MINI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling isuzu - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/isuzu/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[20/29] âœ“ PEUGEOT: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling jeep - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/jeep/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[16/29] âœ“ LANDROVER: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling subaru - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/subaru/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[17/29] âœ“ PORSCHE: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling volkswagen - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/volkswagen/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[18/29] âœ“ VOLVO: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling daewoo - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/daewoo/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[19/29] âœ“ MINI: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling isuzu - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/isuzu/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[20/29] âœ“ PEUGEOT: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling jeep - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/jeep/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[21/29] âœ“ SUBARU: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mg - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mg/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[22/29] âœ“ VOLKSWAGEN: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling baic - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/baic/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[23/29] âœ“ DAEWOO: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling wuling - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/wuling/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[24/29] âœ“ ISUZU: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling byd - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/byd/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[25/29] âœ“ JEEP: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[21/29] âœ“ SUBARU: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling mg - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/mg/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[22/29] âœ“ VOLKSWAGEN: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling baic - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/baic/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[23/29] âœ“ DAEWOO: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling wuling - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/wuling/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[24/29] âœ“ ISUZU: 0 cars\n",
      "\n",
      "============================================================\n",
      "Crawling byd - Max: 10 cars\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://bonbanh.com/oto/byd/page,0\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[25/29] âœ“ JEEP: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[26/29] âœ“ MG: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[27/29] âœ“ BAIC: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[28/29] âœ“ BYD: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[29/29] âœ“ WULING: 0 cars\n",
      "\n",
      "================================================================================\n",
      "BONBANH COMPLETED: 0 total cars from 29 brands\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Crawling Chotot.com - Max: 50 cars\n",
      "Multi-threading: 5 workers\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://xe.chotot.com/mua-ban-oto?page=1\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[26/29] âœ“ MG: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[27/29] âœ“ BAIC: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[28/29] âœ“ BYD: 0 cars\n",
      "No more cars found\n",
      "\n",
      "============================================================\n",
      "Completed: 0 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "[29/29] âœ“ WULING: 0 cars\n",
      "\n",
      "================================================================================\n",
      "BONBANH COMPLETED: 0 total cars from 29 brands\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Crawling Chotot.com - Max: 50 cars\n",
      "Multi-threading: 5 workers\n",
      "CSV: Enabled\n",
      "============================================================\n",
      "\n",
      "Page 1: https://xe.chotot.com/mua-ban-oto?page=1\n",
      "Found 17 cars on page 1\n",
      "Found 17 cars on page 1\n",
      "  [1] âœ“ Crawled\n",
      "  [1] âœ“ Crawled\n",
      "  [2] âœ“ Crawled\n",
      "  [3] âœ“ Crawled\n",
      "  [4] âœ“ Crawled\n",
      "  [2] âœ“ Crawled\n",
      "  [3] âœ“ Crawled\n",
      "  [4] âœ“ Crawled\n",
      "  [5] âœ“ Crawled\n",
      "  [5] âœ“ Crawled\n",
      "  [6] âœ“ Crawled\n",
      "  [7] âœ“ Crawled\n",
      "  [6] âœ“ Crawled\n",
      "  [7] âœ“ Crawled\n",
      "  [8] âœ“ Crawled\n",
      "  [8] âœ“ Crawled\n",
      "  [9] âœ“ Crawled\n",
      "  [10] âœ“ Crawled\n",
      "  [9] âœ“ Crawled\n",
      "  [10] âœ“ Crawled\n",
      "  [11] âœ“ Crawled\n",
      "  [12] âœ“ Crawled\n",
      "  [13] âœ“ Crawled\n",
      "  [11] âœ“ Crawled\n",
      "  [12] âœ“ Crawled\n",
      "  [13] âœ“ Crawled\n",
      "  [14] âœ“ Crawled\n",
      "  [14] âœ“ Crawled\n",
      "  [15] âœ“ Crawled\n",
      "  [15] âœ“ Crawled\n",
      "  [16] âœ“ Crawled\n",
      "  [16] âœ“ Crawled\n",
      "  [17] âœ“ Crawled\n",
      "Page 1 completed: 17/17 cars processed\n",
      "Page 2: https://xe.chotot.com/mua-ban-oto?page=2\n",
      "  [17] âœ“ Crawled\n",
      "Page 1 completed: 17/17 cars processed\n",
      "Page 2: https://xe.chotot.com/mua-ban-oto?page=2\n",
      "Found 18 cars on page 2\n",
      "Found 18 cars on page 2\n",
      "  [18] âœ“ Crawled\n",
      "  [19] âœ“ Crawled\n",
      "  [18] âœ“ Crawled\n",
      "  [19] âœ“ Crawled\n",
      "  [20] âœ“ Crawled\n",
      "  [21] âœ“ Crawled\n",
      "  [22] âœ“ Crawled\n",
      "  [20] âœ“ Crawled\n",
      "  [21] âœ“ Crawled\n",
      "  [22] âœ“ Crawled\n",
      "  [23] âœ“ Crawled\n",
      "  [24] âœ“ Crawled\n",
      "  [23] âœ“ Crawled\n",
      "  [24] âœ“ Crawled\n",
      "  [25] âœ“ Crawled\n",
      "  [26] âœ“ Crawled\n",
      "  [27] âœ“ Crawled\n",
      "  [28] âœ“ Crawled\n",
      "  [25] âœ“ Crawled\n",
      "  [26] âœ“ Crawled\n",
      "  [27] âœ“ Crawled\n",
      "  [28] âœ“ Crawled\n",
      "  [29] âœ“ Crawled\n",
      "  [29] âœ“ Crawled\n",
      "  [30] âœ“ Crawled\n",
      "  [30] âœ“ Crawled\n",
      "  [31] âœ“ Crawled\n",
      "  [31] âœ“ Crawled\n",
      "  [32] âœ“ Crawled\n",
      "  [33] âœ“ Crawled\n",
      "  [32] âœ“ Crawled\n",
      "  [33] âœ“ Crawled\n",
      "  [34] âœ“ Crawled\n",
      "Page 2 completed: 17/18 cars processed\n",
      "Page 3: https://xe.chotot.com/mua-ban-oto?page=3\n",
      "  [34] âœ“ Crawled\n",
      "Page 2 completed: 17/18 cars processed\n",
      "Page 3: https://xe.chotot.com/mua-ban-oto?page=3\n",
      "Found 17 cars on page 3\n",
      "Found 17 cars on page 3\n",
      "  [35] âœ“ Crawled\n",
      "  [36] âœ“ Crawled\n",
      "  [35] âœ“ Crawled\n",
      "  [36] âœ“ Crawled\n",
      "  [37] âœ“ Crawled\n",
      "  [38] âœ“ Crawled\n",
      "  [39] âœ“ Crawled\n",
      "  [37] âœ“ Crawled\n",
      "  [38] âœ“ Crawled\n",
      "  [39] âœ“ Crawled\n",
      "  [40] âœ“ Crawled\n",
      "  [41] âœ“ Crawled\n",
      "  [40] âœ“ Crawled\n",
      "  [41] âœ“ Crawled\n",
      "  [42] âœ“ Crawled\n",
      "  [43] âœ“ Crawled\n",
      "  [44] âœ“ Crawled\n",
      "  [42] âœ“ Crawled\n",
      "  [43] âœ“ Crawled\n",
      "  [44] âœ“ Crawled\n",
      "  [45] âœ“ Crawled\n",
      "  [46] âœ“ Crawled\n",
      "  [47] âœ“ Crawled\n",
      "  [45] âœ“ Crawled\n",
      "  [46] âœ“ Crawled\n",
      "  [47] âœ“ Crawled\n",
      "  [48] âœ“ Crawled\n",
      "  [48] âœ“ Crawled\n",
      "  [49] âœ“ Crawled\n",
      "  [49] âœ“ Crawled\n",
      "  [50] âœ“ Crawled\n",
      "Page 3 completed: 16/16 cars processed\n",
      "\n",
      "============================================================\n",
      "Completed: 50 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FULL CRAWL COMPLETED\n",
      "  Bonbanh: 0 cars\n",
      "  Chotot: 50 cars\n",
      "  TOTAL: 50 cars\n",
      "================================================================================\n",
      "\n",
      "  [50] âœ“ Crawled\n",
      "Page 3 completed: 16/16 cars processed\n",
      "\n",
      "============================================================\n",
      "Completed: 50 cars\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FULL CRAWL COMPLETED\n",
      "  Bonbanh: 0 cars\n",
      "  Chotot: 50 cars\n",
      "  TOTAL: 50 cars\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This will crawl ALL brands with default limits\n",
    "# Bonbanh: 500 cars per brand * 29 brands (Some brands may not have enough 500 cars)\n",
    "# Chotot: 5,000 cars\n",
    "# Multi-threading: Crawls multiple brands in parallel for faster execution\n",
    "\n",
    "# Helper function for parallel brand crawling\n",
    "def crawl_brand_worker(brand: str, max_cars: int = 500):\n",
    "    \"\"\"Worker function for parallel crawling.\"\"\"\n",
    "    crawler = BonbanhCrawler(delay=0.0)\n",
    "    return brand, crawler.crawl_brand(brand, max_cars=max_cars)\n",
    "\n",
    "# Crawl all Bonbanh brands with multi-threading\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING FULL CRAWL - {len(CAR_BRANDS)} BRANDS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "total_cars_bonbanh = 0\n",
    "max_workers = 5  # Number of parallel threads (adjust based on your system)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit all brand crawling tasks\n",
    "    future_to_brand = {\n",
    "        executor.submit(crawl_brand_worker, brand, 10): brand \n",
    "        for brand in CAR_BRANDS\n",
    "    }\n",
    "    \n",
    "    # Process completed tasks as they finish\n",
    "    for i, future in enumerate(concurrent.futures.as_completed(future_to_brand), 1):\n",
    "        brand = future_to_brand[future]\n",
    "        try:\n",
    "            brand_name, count = future.result()\n",
    "            total_cars_bonbanh += count\n",
    "            print(f\"\\n[{i}/{len(CAR_BRANDS)}] âœ“ {brand_name.upper()}: {count} cars\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[{i}/{len(CAR_BRANDS)}] âœ— {brand.upper()}: Error - {e}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BONBANH COMPLETED: {total_cars_bonbanh} total cars from {len(CAR_BRANDS)} brands\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Crawl Chotot (single-threaded as it's one source)\n",
    "chotot_crawler = ChototCrawler(delay=0.0, max_workers=5)\n",
    "total_cars_chotot = chotot_crawler.crawl_listings(max_cars=50)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FULL CRAWL COMPLETED\")\n",
    "print(f\"  Bonbanh: {total_cars_bonbanh} cars\")  \n",
    "print(f\"  Chotot: {total_cars_chotot} cars\")\n",
    "print(f\"  TOTAL: {total_cars_bonbanh + total_cars_chotot} cars\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
